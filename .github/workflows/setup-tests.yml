name: Test Configuration Setup

on:
  workflow_dispatch:
  push:
    branches: [ main ]

env:
  JAVA_VERSION: '17'
  NODE_VERSION: '18'

jobs:
  setup-backend-tests:
    name: Setup Backend Test Configuration
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [userservice, productservice, orderservice, inventoryservice, supplierservice, resourceservice, notificationservice]
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Set up JDK ${{ env.JAVA_VERSION }}
      uses: actions/setup-java@v4
      with:
        java-version: ${{ env.JAVA_VERSION }}
        distribution: 'temurin'
        
    - name: Create Test Configuration for ${{ matrix.service }}
      working-directory: ./backend/${{ matrix.service }}
      run: |
        # Create test application properties
        cat > src/test/resources/application-test.properties << EOF
        # Test Configuration for ${{ matrix.service }}
        spring.application.name=${{ matrix.service }}-test
        server.port=0
        
        # Test Database Configuration
        spring.datasource.url=jdbc:h2:mem:testdb
        spring.datasource.driver-class-name=org.h2.Driver
        spring.datasource.username=sa
        spring.datasource.password=
        
        # JPA Configuration
        spring.jpa.hibernate.ddl-auto=create-drop
        spring.jpa.show-sql=false
        spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.H2Dialect
        
        # Disable external services for testing
        stripe.secret.key=sk_test_fake_key_for_testing
        stripe.api.key=sk_test_fake_key_for_testing
        cloudinary.cloud-name=test-cloud
        cloudinary.api-key=test-key
        cloudinary.api-secret=test-secret
        
        # Logging Configuration
        logging.level.org.springframework=WARN
        logging.level.com.${{ matrix.service }}=DEBUG
        
        # Test Profile
        spring.profiles.active=test
        EOF
        
        echo "Test configuration created for ${{ matrix.service }}"
        
    - name: Add Test Dependencies
      working-directory: ./backend/${{ matrix.service }}
      run: |
        # Add H2 database for testing
        mvn dependency:resolve-sources
        echo "Dependencies resolved for ${{ matrix.service }}"

  setup-frontend-tests:
    name: Setup Frontend Test Configuration
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: './frontend/inventory-management-system/package-lock.json'
        
    - name: Create Test Configuration
      working-directory: ./frontend/inventory-management-system
      run: |
        # Create test environment file
        cat > .env.test << EOF
        # Test Environment Configuration
        NEXT_PUBLIC_API_BASE_URL=http://localhost:8090
        NEXT_PUBLIC_USER_SERVICE_URL=http://localhost:8080
        NEXT_PUBLIC_PRODUCT_SERVICE_URL=http://localhost:8083
        NEXT_PUBLIC_ORDER_SERVICE_URL=http://localhost:8084
        NEXT_PUBLIC_INVENTORY_SERVICE_URL=http://localhost:8085
        NEXT_PUBLIC_SUPPLIER_SERVICE_URL=http://localhost:8082
        NEXT_PUBLIC_RESOURCE_SERVICE_URL=http://localhost:8086
        NEXT_PUBLIC_NOTIFICATION_SERVICE_URL=http://localhost:8087
        
        # Test Mode
        NODE_ENV=test
        NEXT_PUBLIC_TEST_MODE=true
        EOF
        
        echo "Frontend test configuration created"
        
    - name: Install Test Dependencies
      working-directory: ./frontend/inventory-management-system
      run: |
        npm install --save-dev @testing-library/react @testing-library/jest-dom jest jest-environment-jsdom
        echo "Test dependencies installed"

  create-test-scripts:
    name: Create Test Scripts
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Create Backend Test Scripts
      run: |
        # Create test runner script for backend services
        cat > scripts/run-backend-tests.sh << 'EOF'
        #!/bin/bash
        # Backend Test Runner Script
        
        echo "Starting backend tests..."
        
        SERVICES=("userservice" "productservice" "orderservice" "inventoryservice" "supplierservice" "resourceservice" "notificationservice")
        
        for service in "${SERVICES[@]}"; do
          echo "Testing $service..."
          cd "backend/$service"
          
          # Run unit tests
          mvn test -Dspring.profiles.active=test
          
          # Run integration tests
          mvn test -Dtest=*IntegrationTest -Dspring.profiles.active=test
          
          # Generate coverage report
          mvn jacoco:report
          
          cd ../..
          echo "$service tests completed"
        done
        
        echo "All backend tests completed!"
        EOF
        
        chmod +x scripts/run-backend-tests.sh
        
        # Create frontend test script
        cat > scripts/run-frontend-tests.sh << 'EOF'
        #!/bin/bash
        # Frontend Test Runner Script
        
        echo "Starting frontend tests..."
        
        cd frontend/inventory-management-system
        
        # Install dependencies
        npm ci
        
        # Run unit tests
        npm run test:ci
        
        # Run integration tests
        npm run test:integration
        
        # Generate coverage report
        npm run test:coverage
        
        cd ../..
        echo "Frontend tests completed!"
        EOF
        
        chmod +x scripts/run-frontend-tests.sh
        
        # Create performance test script
        cat > scripts/run-performance-tests.sh << 'EOF'
        #!/bin/bash
        # Performance Test Runner Script
        
        echo "Starting performance tests..."
        
        # Install Apache Bench if not available
        if ! command -v ab &> /dev/null; then
          sudo apt-get update
          sudo apt-get install -y apache2-utils
        fi
        
        # Test API endpoints
        SERVICES=("userservice:8080" "productservice:8083" "orderservice:8084" "inventoryservice:8085")
        
        for service_port in "${SERVICES[@]}"; do
          IFS=':' read -r service port <<< "$service_port"
          echo "Testing $service performance..."
          
          ab -n 1000 -c 10 "http://localhost:$port/api/$service/health" > "performance-$service.log" 2>&1
          
          echo "$service performance test completed"
        done
        
        echo "All performance tests completed!"
        EOF
        
        chmod +x scripts/run-performance-tests.sh
        
        echo "Test scripts created successfully"

  create-docker-test-environment:
    name: Create Docker Test Environment
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Create Docker Compose for Testing
      run: |
        # Create docker-compose for test environment
        cat > docker-compose.test.yml << 'EOF'
        version: '3.8'
        
        services:
          test-mysql:
            image: mysql:8.0
            container_name: test-mysql
            environment:
              MYSQL_ROOT_PASSWORD: testpass
              MYSQL_DATABASE: testdb
              MYSQL_USER: testuser
              MYSQL_PASSWORD: testpass
            ports:
              - "3306:3306"
            volumes:
              - test-mysql-data:/var/lib/mysql
            healthcheck:
              test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
              timeout: 20s
              retries: 10
        
          test-redis:
            image: redis:7-alpine
            container_name: test-redis
            ports:
              - "6379:6379"
            volumes:
              - test-redis-data:/data
            healthcheck:
              test: ["CMD", "redis-cli", "ping"]
              timeout: 20s
              retries: 10
        
          test-kafka:
            image: confluentinc/cp-kafka:latest
            container_name: test-kafka
            environment:
              KAFKA_ZOOKEEPER_CONNECT: test-zookeeper:2181
              KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
              KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            ports:
              - "9092:9092"
            depends_on:
              - test-zookeeper
        
          test-zookeeper:
            image: confluentinc/cp-zookeeper:latest
            container_name: test-zookeeper
            environment:
              ZOOKEEPER_CLIENT_PORT: 2181
              ZOOKEEPER_TICK_TIME: 2000
            ports:
              - "2181:2181"
        
        volumes:
          test-mysql-data:
          test-redis-data:
        EOF
        
        echo "Docker test environment configuration created"

  setup-notification-config:
    name: Setup Notification Configuration
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Create Notification Templates
      run: |
        # Create email notification template
        cat > scripts/email-template.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
          <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            .header { background-color: #f4f4f4; padding: 20px; border-radius: 5px; }
            .content { margin: 20px 0; }
            .success { color: #28a745; }
            .error { color: #dc3545; }
            .warning { color: #ffc107; }
            .footer { margin-top: 30px; font-size: 12px; color: #666; }
          </style>
        </head>
        <body>
          <div class="header">
            <h2>Daily Test Results - Inventory Management System</h2>
            <p>Date: {{DATE}}</p>
          </div>
          
          <div class="content">
            <h3>Test Summary</h3>
            <ul>
              <li class="success">âœ… Backend Services: {{BACKEND_STATUS}}</li>
              <li class="success">âœ… Frontend Tests: {{FRONTEND_STATUS}}</li>
              <li class="success">âœ… External Services: {{EXTERNAL_STATUS}}</li>
              <li class="success">âœ… Performance Tests: {{PERFORMANCE_STATUS}}</li>
            </ul>
            
            <h3>Coverage Summary</h3>
            <p>Overall Coverage: <strong>{{COVERAGE}}%</strong></p>
            
            <h3>Performance Metrics</h3>
            <ul>
              <li>API Response Time: {{API_TIME}}ms</li>
              <li>Database Query Time: {{DB_TIME}}ms</li>
              <li>Error Rate: {{ERROR_RATE}}%</li>
            </ul>
          </div>
          
          <div class="footer">
            <p>This is an automated report from the CI/CD pipeline.</p>
            <p>View detailed results: {{REPORT_URL}}</p>
          </div>
        </body>
        </html>
        EOF
        
        # Create Slack notification template
        cat > scripts/slack-template.json << 'EOF'
        {
          "text": "ðŸš€ Daily Test Pipeline Results",
          "blocks": [
            {
              "type": "section",
              "text": {
                "type": "mrkdwn",
                "text": "*Daily Test Results - {{DATE}}*"
              }
            },
            {
              "type": "section",
              "fields": [
                {
                  "type": "mrkdwn",
                  "text": "*Backend Services*\n{{BACKEND_STATUS}}"
                },
                {
                  "type": "mrkdwn",
                  "text": "*Frontend Tests*\n{{FRONTEND_STATUS}}"
                },
                {
                  "type": "mrkdwn",
                  "text": "*External Services*\n{{EXTERNAL_STATUS}}"
                },
                {
                  "type": "mrkdwn",
                  "text": "*Performance Tests*\n{{PERFORMANCE_STATUS}}"
                }
              ]
            },
            {
              "type": "section",
              "text": {
                "type": "mrkdwn",
                "text": "*Coverage: {{COVERAGE}}%* | *API Response: {{API_TIME}}ms* | *Error Rate: {{ERROR_RATE}}%*"
              }
            },
            {
              "type": "actions",
              "elements": [
                {
                  "type": "button",
                  "text": {
                    "type": "plain_text",
                    "text": "View Full Report"
                  },
                  "url": "{{REPORT_URL}}"
                }
              ]
            }
          ]
        }
        EOF
        
        echo "Notification templates created successfully"

  create-test-documentation:
    name: Create Test Documentation
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Create Test Documentation
      run: |
        # Create comprehensive test documentation
        cat > docs/TESTING_GUIDE.md << 'EOF'
        # Testing Guide - Inventory Management System
        
        ## Overview
        This document describes the comprehensive testing strategy for the Inventory Management System, including daily automated testing, code coverage analysis, and performance regression testing.
        
        ## Test Types
        
        ### 1. Unit Tests
        - **Backend**: JUnit tests for all microservices
        - **Frontend**: Jest tests for React components
        - **Coverage Target**: 85% minimum
        
        ### 2. Integration Tests
        - **API Integration**: Test service-to-service communication
        - **Database Integration**: Test database operations
        - **External Service Integration**: Test Stripe, Cloudinary APIs
        
        ### 3. Performance Tests
        - **Load Testing**: Apache Bench for API endpoints
        - **Database Performance**: Query execution time monitoring
        - **External Service Performance**: Response time monitoring
        
        ## Daily Test Schedule
        
        | Time (UTC) | Test Type | Duration |
        |------------|-----------|----------|
        | 2:00 AM | Full Test Suite | 45 minutes |
        | 3:00 AM | Code Coverage Analysis | 30 minutes |
        | 4:00 AM | Performance Regression | 60 minutes |
        
        ## Test Configuration
        
        ### Backend Services
        Each microservice has dedicated test configuration:
        - Test database (H2 in-memory)
        - Mock external services
        - Test-specific application properties
        
        ### Frontend
        - Test environment variables
        - Mock API responses
        - Component testing setup
        
        ## Coverage Requirements
        
        | Service | Minimum Coverage | Target Coverage |
        |---------|----------------|-----------------|
        | User Service | 80% | 90% |
        | Product Service | 80% | 90% |
        | Order Service | 85% | 95% |
        | Inventory Service | 80% | 90% |
        | Supplier Service | 75% | 85% |
        | Resource Service | 75% | 85% |
        | Notification Service | 75% | 85% |
        | Frontend | 75% | 85% |
        
        ## Performance Thresholds
        
        | Metric | Threshold | Critical Threshold |
        |--------|-----------|-------------------|
        | API Response Time | < 200ms | < 500ms |
        | Database Query Time | < 100ms | < 200ms |
        | Stripe API Response | < 500ms | < 1000ms |
        | Cloudinary Response | < 300ms | < 600ms |
        | Error Rate | < 1% | < 5% |
        
        ## Running Tests Locally
        
        ### Backend Tests
        ```bash
        # Run all backend tests
        ./scripts/run-backend-tests.sh
        
        # Run specific service tests
        cd backend/userservice
        mvn test -Dspring.profiles.active=test
        ```
        
        ### Frontend Tests
        ```bash
        # Run all frontend tests
        ./scripts/run-frontend-tests.sh
        
        # Run specific test types
        cd frontend/inventory-management-system
        npm run test:unit
        npm run test:integration
        npm run test:coverage
        ```
        
        ### Performance Tests
        ```bash
        # Run performance tests
        ./scripts/run-performance-tests.sh
        ```
        
        ## Test Environment Setup
        
        ### Docker Environment
        ```bash
        # Start test environment
        docker-compose -f docker-compose.test.yml up -d
        
        # Wait for services to be ready
        sleep 30
        ```
        
        ### Manual Setup
        1. Start MySQL test database
        2. Start Redis cache
        3. Start Kafka (if needed)
        4. Configure test environment variables
        
        ## Monitoring and Alerts
        
        ### Success Criteria
        - All tests pass
        - Coverage thresholds met
        - Performance thresholds met
        - No regressions detected
        
        ### Failure Handling
        - Automatic retry for flaky tests
        - Email notifications for failures
        - Slack alerts for critical issues
        - Detailed failure reports
        
        ## Troubleshooting
        
        ### Common Issues
        1. **Database Connection**: Check MySQL service status
        2. **External Service Timeouts**: Verify API keys and network
        3. **Memory Issues**: Increase JVM heap size
        4. **Test Timeouts**: Increase test timeout values
        
        ### Debug Commands
        ```bash
        # Check service health
        curl http://localhost:8080/api/users/health
        
        # Check database connection
        mysql -h localhost -u testuser -ptestpass testdb
        
        # Check Redis connection
        redis-cli ping
        ```
        
        ## Best Practices
        
        1. **Test Isolation**: Each test should be independent
        2. **Mock External Services**: Use mocks for external APIs
        3. **Test Data**: Use consistent test data sets
        4. **Cleanup**: Clean up test data after each test
        5. **Documentation**: Document test cases and scenarios
        
        ## Continuous Improvement
        
        - Regular review of test coverage
        - Performance baseline updates
        - Test execution time optimization
        - New test case additions
        EOF
        
        echo "Test documentation created successfully"

  finalize-setup:
    name: Finalize Test Setup
    runs-on: ubuntu-latest
    needs: [setup-backend-tests, setup-frontend-tests, create-test-scripts, create-docker-test-environment, setup-notification-config, create-test-documentation]
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Create Setup Summary
      run: |
        echo "# Test Setup Summary - $(date)" > test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "## Setup Completed Successfully âœ…" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "### Backend Test Configuration" >> test-setup-summary.md
        echo "- Test properties created for all 7 microservices" >> test-setup-summary.md
        echo "- H2 database configuration added" >> test-setup-summary.md
        echo "- Mock external service configurations" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "### Frontend Test Configuration" >> test-setup-summary.md
        echo "- Test environment variables configured" >> test-setup-summary.md
        echo "- Testing library dependencies installed" >> test-setup-summary.md
        echo "- Jest configuration updated" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "### Test Scripts Created" >> test-setup-summary.md
        echo "- Backend test runner script" >> test-setup-summary.md
        echo "- Frontend test runner script" >> test-setup-summary.md
        echo "- Performance test runner script" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "### Docker Test Environment" >> test-setup-summary.md
        echo "- MySQL test database container" >> test-setup-summary.md
        echo "- Redis cache container" >> test-setup-summary.md
        echo "- Kafka message queue container" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "### Notification Templates" >> test-setup-summary.md
        echo "- Email notification template" >> test-setup-summary.md
        echo "- Slack notification template" >> test-setup-summary.md
        echo "- Report generation scripts" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "### Documentation" >> test-setup-summary.md
        echo "- Comprehensive testing guide created" >> test-setup-summary.md
        echo "- Troubleshooting documentation" >> test-setup-summary.md
        echo "- Best practices guide" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "## Next Steps" >> test-setup-summary.md
        echo "1. Configure GitHub Secrets for external service API keys" >> test-setup-summary.md
        echo "2. Set up email and Slack webhook URLs" >> test-setup-summary.md
        echo "3. Run initial test to verify setup" >> test-setup-summary.md
        echo "4. Monitor daily test execution" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "## Daily Test Schedule" >> test-setup-summary.md
        echo "- 2:00 AM UTC: Full test suite execution" >> test-setup-summary.md
        echo "- 3:00 AM UTC: Code coverage analysis" >> test-setup-summary.md
        echo "- 4:00 AM UTC: Performance regression testing" >> test-setup-summary.md
        echo "" >> test-setup-summary.md
        echo "Setup completed successfully! ðŸŽ‰" >> test-setup-summary.md
        
    - name: Upload Setup Summary
      uses: actions/upload-artifact@v3
      with:
        name: test-setup-summary
        path: test-setup-summary.md
        retention-days: 30
